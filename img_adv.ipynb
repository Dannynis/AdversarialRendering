{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls /home/dcor/niskhizov/ | grep gan_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "tt = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_index = 0\n",
    "\n",
    "cap = cv2.VideoCapture(camera_index, cv2.CAP_DSHOW) # this is the magic!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(img,c='red'):\n",
    "    clear_output()\n",
    "    plt.imshow(img.transpose(1,0,2))\n",
    "    plt.axis('off')\n",
    "    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "    plt.gcf().set_facecolor(c)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(5):\n",
    "    ret, frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to grab frame\")\n",
    "else:\n",
    "    orig_ref_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tqdm \n",
    "\n",
    "secondary_screen_width, secondary_screen_height = 600, 300 \n",
    "scale = 1\n",
    "\n",
    "pattern = np.zeros((secondary_screen_width//scale, secondary_screen_height//scale,3),dtype=np.uint8) \n",
    "\n",
    "caps_x = []\n",
    "caps_y = []\n",
    "\n",
    "jumps = 10\n",
    "\n",
    "\n",
    "plot_img(pattern)\n",
    "\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print('capturing base')\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "for k in range(5):\n",
    "    ret, frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to grab frame\")\n",
    "else:\n",
    "    orig_ref_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "clear_output()\n",
    "print('capturing black base')\n",
    "pattern = np.zeros((secondary_screen_width//scale, secondary_screen_height//scale,3),dtype=np.uint8) \n",
    "plot_img(pattern)\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "for k in range(5):\n",
    "    ret, frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to grab frame\")\n",
    "else:\n",
    "    orig_ref_frame_black = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "\n",
    "for i in tqdm.tqdm(range(0,pattern.shape[0],jumps)):\n",
    "    pattern = np.zeros((secondary_screen_width//scale, secondary_screen_height//scale,3),dtype=np.uint8) \n",
    "    pattern[i:i+1,:,:] = 255\n",
    "    \n",
    "    plot_img(pattern,'black')\n",
    "\n",
    "    time.sleep(0.1)\n",
    "    for k in range(5):\n",
    "        ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "    else:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        caps_x.append(frame)\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm.tqdm(range(0,pattern.shape[1],jumps)):\n",
    "    pattern = np.zeros((secondary_screen_width//scale, secondary_screen_height//scale,3),dtype=np.uint8) \n",
    "    pattern[:,i:i+1,:] = 255\n",
    "    \n",
    "    plot_img(pattern,'black')\n",
    "\n",
    "    time.sleep(0.1)\n",
    "    for k in range(5):\n",
    "        ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "    else:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        caps_y.append(frame)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_rectangles(frame):\n",
    "    # blurred = cv2.GaussianBlur(frame, (5, 5), 0)\n",
    "    # edges = cv2.Canny(frame, 50, 150)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(frame, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    rectangles = []\n",
    "    for contour in contours:\n",
    "        # Approximate the contour\n",
    "        epsilon = 0.02 * cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "\n",
    "        # Check if it's a rectangle\n",
    "        if len(approx) == 4 and cv2.isContourConvex(approx):\n",
    "            # Calculate the aspect ratio and area\n",
    "            area = cv2.contourArea(approx)\n",
    "            if area > 100:  # Filter small shapes\n",
    "                # check if the rectangle is black\n",
    "                x, y, w, h = cv2.boundingRect(approx)\n",
    "                # chuck if rectangles lines are white\n",
    "                if frame[y:y+h, x:x+w].mean() < 250:\n",
    "                    rectangles.append(approx)\n",
    "        # return the bigest rectangle\n",
    "    if len(rectangles) > 0:\n",
    "        rectangles = max(rectangles, key=cv2.contourArea)\n",
    "    else:\n",
    "        rectangles = []\n",
    "        \n",
    "    return rectangles    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = copy.deepcopy(orig_ref_frame_black)\n",
    "red_mask = (orig_ref_frame_black[:,:,0] > 230).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(red_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rect = find_rectangles(red_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frame)\n",
    "# add a rectangle to the image\n",
    "cv2.drawContours(frame, rect, -1, (0, 255, 0), 3)\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin =  4\n",
    "max_w = rect[:,0,:][:,0].max() - margin\n",
    "min_w = rect[:,0,:][:,0].min() + margin\n",
    "max_h = rect[:,0,:][:,1].max() - margin\n",
    "min_h = rect[:,0,:][:,1].min() + margin \n",
    "ref_frame = cv2.cvtColor(frame[min_h:max_h,min_w:max_w], cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ref_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = []\n",
    "counter = 0\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "pbar = tqdm(total=len(caps_x)*len(caps_y))\n",
    "\n",
    "edges_ref = cv2.Canny(ref_frame, 50, 150, apertureSize=3)\n",
    "\n",
    "for x in range(len(caps_x)):\n",
    "    for y in range(len(caps_y)):   \n",
    "        #find vertical line in caps_x and horizontal line in caps_y\n",
    "\n",
    "        rect_caps_x = cv2.cvtColor( caps_x[x][min_h:max_h,min_w:max_w,:], cv2.COLOR_BGR2GRAY)\n",
    "        rect_caps_y = cv2.cvtColor( caps_y[y][min_h:max_h,min_w:max_w,:], cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # d_x = rect_caps_x-ref_frame\n",
    "\n",
    "        # d_x = d_x* ( (d_x < 200) * (d_x > 10))\n",
    "\n",
    "        # d_y = rect_caps_y-ref_frame\n",
    "\n",
    "        # d_y = d_y* ( (d_y < 200) * (d_y > 10))\n",
    "    \n",
    "        edges_x = cv2.Canny(rect_caps_x, 50, 150, apertureSize=3)\n",
    "        edges_y = cv2.Canny(rect_caps_y, 50, 150, apertureSize=3)\n",
    "        if edges_x.max() != 255 or edges_y.max() != 255:\n",
    "            # print('bad frame')\n",
    "            continue\n",
    "        \n",
    "        # idx_x, idx_y = np.array(((edges_x == 255) * (edges_y == 255)).nonzero()).T.mean(0).astype(int)\n",
    "        lines = cv2.HoughLinesP(image=edges_y-edges_ref,rho=1,theta=np.pi/180, threshold=20,minLineLength=10)#,lines=np.array([]), minLineLength=1,maxLineGap=80)\n",
    "        if lines is None:\n",
    "            print ('no y lines')\n",
    "            if y > 10:\n",
    "                raise\n",
    "            continue\n",
    "        a,b,c = lines.shape\n",
    "        edges_y_l = copy.deepcopy(edges_y)*0\n",
    "        for i in range(a):\n",
    "            cv2.line(edges_y_l, (0, lines[i][0][1]), (edges_y.shape[1], lines[i][0][3]), 255, 1, cv2.LINE_AA)\n",
    "\n",
    "        lines = cv2.HoughLinesP(image=edges_x-edges_ref,rho=1,theta=np.pi/180, threshold=20,minLineLength=10)#,lines=np.array([]), minLineLength=1,maxLineGap=80)\n",
    "        if lines is None:\n",
    "            print ('no x lines')\n",
    "            continue\n",
    "        a,b,c = lines.shape\n",
    "        edges_x_l = copy.deepcopy(edges_x)*0\n",
    "        for i in range(a):\n",
    "            cv2.line(edges_x_l, (lines[i][0][0], 0), (lines[i][0][2],edges_x.shape[0]), 255, 1, cv2.LINE_AA)\n",
    "\n",
    "        edges_cut = (edges_x_l* edges_y_l).argmax()\n",
    "        edges_cut_idx = np.unravel_index(edges_cut,edges_x_l.shape)\n",
    "\n",
    "        idx_x, idx_y = edges_cut_idx\n",
    "\n",
    "        if idx_x < 0 or idx_y < 0:\n",
    "            continue\n",
    "\n",
    "      \n",
    "        orig_idx_y = idx_x + min_h\n",
    "        orig_idx_x = idx_y + min_w\n",
    "        comb = edges_x + edges_y\n",
    "        positions.append(((x * jumps,y * jumps),(orig_idx_x,orig_idx_y)))\n",
    "        counter +=1 \n",
    "        pbar.update(1)\n",
    "        # if counter % 1000 == 0:\n",
    "        #     plt.imshow(comb)\n",
    "        #     plt.scatter(idx_y,idx_x)\n",
    "        #     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = cv2.HoughLinesP(image=edges_y,rho=1,theta=np.pi/180, threshold=10,minLineLength=20)#,lines=np.array([]), minLineLength=1,maxLineGap=80)\n",
    "\n",
    "a,b,c = lines.shape\n",
    "edges_y_l = copy.deepcopy(edges_y)*0\n",
    "for i in range(a):\n",
    "    cv2.line(edges_y_l, (0, lines[i][0][1]), (edges_y.shape[1], lines[i][0][3]), 255, 1, cv2.LINE_AA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# H, _ = cv2.findHomography(original_points, new_points, cv2.RANSAC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_points = np.array([x[0] for x in positions])\n",
    "new_points = np.array([x[1] for x in positions])\n",
    "\n",
    "h_scale = np.zeros(new_points[:,0].max()+1)\n",
    "w_scale = np.zeros(new_points[:,1].max()+1)\n",
    "balanced_new_points = []\n",
    "balanced_original_points = []\n",
    "for i in range(len(new_points)):\n",
    "    x,y = new_points[i]\n",
    "    if h_scale[x] > 1000:\n",
    "        continue\n",
    "    if w_scale[y] > 1000:\n",
    "        continue\n",
    "    h_scale[x] += 1\n",
    "    w_scale[y] += 1\n",
    "    balanced_new_points.append((x,y))\n",
    "    balanced_original_points.append(original_points[i])\n",
    "\n",
    "balanced_new_points = np.array(balanced_new_points)\n",
    "balanced_original_points = np.array(balanced_original_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(balanced_new_points[:,0],bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, _ = cv2.findHomography(balanced_original_points, balanced_new_points, cv2.RANSAC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_points[:,1].max())\n",
    "print(new_points[:,1].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_new_points[:,0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Warp original points\n",
    "warped_points_pred = cv2.perspectiveTransform(original_points.reshape(-1, 1, 2).astype(np.float32), H)\n",
    "\n",
    "# Plot on the captured image\n",
    "# plt.imshow(new_points, cmap='gray')\n",
    "plt.imshow(caps_y[0])\n",
    "plt.scatter(balanced_new_points[:, 0], balanced_new_points[:, 1], c='blue', label='Ground Truth', s=0.05)\n",
    "\n",
    "plt.scatter(warped_points_pred[:, 0, 0], warped_points_pred[:, 0, 1], c='red', label='Predicted', s=0.05)\n",
    "plt.legend()\n",
    "plt.title('Ground Truth vs Predicted Points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_dst = cv2.warpPerspective(   255+0*pattern.transpose(1,0,2), H, (frame.T.shape[1], frame.T.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_dst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)*0.5+im_dst[:,:,0]*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./calibration_dump',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump and dump orig_ref_frame\n",
    "import pickle\n",
    "with open('./calibration_dump/calibration2.pickle','wb') as f:\n",
    "    pickle.dump(H,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(orig_ref_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./calibration_dump/orig_ref_frame2.pickle','wb') as f:\n",
    "    pickle.dump(orig_ref_frame,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/dcor/niskhizov/AdversarialRendering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "class LoRA_U_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LoRA_U_Net, self).__init__()\n",
    "        self.unet = models.segmentation.deeplabv3_mobilenet_v3_large(pretrained=True)\n",
    "        \n",
    "        # Modify classifier for image prediction task (3 output channels for RGB)\n",
    "        self.unet.classifier[4] = nn.Conv2d(256, 3, kernel_size=1)\n",
    "\n",
    "        # # Apply LoRA to the convolution layers\n",
    "        # config = LoraConfig(\n",
    "        #     r=8,  # Low-rank parameter\n",
    "        #     lora_alpha=16,\n",
    "        #     lora_dropout=0.05,\n",
    "        #     target_modules=[\"conv1\"]  # Apply LoRA to deeper layers\n",
    "        # )\n",
    "\n",
    "        # self.unet = get_peft_model(self.unet, config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = self.unet(x)['out'] + x  # Residual Learning\n",
    "        # o = (o - o.min()) / (o.max() - o.min())  # Normalize to [0, 1]\n",
    "        return o.clamp(0,1)\n",
    "\n",
    "# Load model\n",
    "net = LoRA_U_Net().cuda()\n",
    "# Load weights\n",
    "net.load_state_dict(torch.load(\"/home/dcor/niskhizov/lora_mobile_unet4.pth\"))\n",
    "\n",
    "net = net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "tt = transforms.ToTensor()\n",
    "# load data from calibration\n",
    "with open('./calibration_dump/calibration2.pickle','rb') as f:\n",
    "    H = pickle.load(f)\n",
    "with open('./calibration_dump/orig_ref_frame2.pickle','rb') as f:\n",
    "    orig_ref_frame = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_ref_frame_tensor = tt(orig_ref_frame).unsqueeze(0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_ref_frame_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = net(orig_ref_frame_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(orig_ref_frame_tensor.cpu().detach().squeeze().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(net(orig_ref_frame_tensor).cpu().detach().squeeze().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(o.cpu().detach().numpy().squeeze().transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import kornia\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# Load stable diffusion model\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "vae=  pipe.vae\n",
    "vae = vae.to(device)\n",
    "def decode_latents(latents):\n",
    "    # latents = F.interpolate(latents, (64, 64), mode='bilinear', align_corners=False)\n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast(device):\n",
    "            latents = 1 / 0.18215 * latents\n",
    "\n",
    "            with torch.no_grad():\n",
    "                imgs = vae.decode(latents).sample\n",
    "\n",
    "            # imgs = (imgs / 2 + 0.5).clamp(0, 1)\n",
    "\n",
    "    return imgs\n",
    "\n",
    "def encode_imgs(imgs):\n",
    "    # imgs: [B, 3, H, W]\n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast(device):\n",
    "            # imgs = 2 * imgs - 1\n",
    "\n",
    "            posterior = vae.encode(imgs).latent_dist\n",
    "            latents = posterior.sample() * 0.18215\n",
    "\n",
    "    return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.classfier import *\n",
    "# from utils.gan import *\n",
    "# net = Generator()\n",
    "# # net.load_state_dict(torch.load(\"./calibration_dump//patch_patterns_gan_unet_31.pth\"))\n",
    "# net.load_state_dict(torch.load(\"/home/dcor/niskhizov/patch_caltec_patterns_gan_unet_140.pth\"))\n",
    "\n",
    "net = net.eval().cuda()\n",
    "# freeze the model\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "def gaussian_kernel(size: int, sigma: float):\n",
    "    \"\"\"Generates a 2D Gaussian kernel.\"\"\"\n",
    "    coords = torch.arange(size).float()\n",
    "    coords -= (size - 1) / 2.0\n",
    "    g = torch.exp(-coords.pow(2) / (2 * sigma ** 2))\n",
    "    g = g / g.sum()  # Normalize\n",
    "    g_2d = g.unsqueeze(0) * g.unsqueeze(1)  # Outer product to create 2D kernel\n",
    "    return g_2d\n",
    "\n",
    "# Create Gaussian kernel\n",
    "kernel_size = 71  # Kernel size (odd number)\n",
    "sigma = 70        # Standard deviation of the Gaussian\n",
    "num_noises = 1\n",
    "gaussian_filter = gaussian_kernel(kernel_size, sigma).unsqueeze(0).unsqueeze(0).repeat(3,1,1,1)\n",
    "\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=kernel_size,\n",
    "                       padding=kernel_size // 2, bias=False,\n",
    "                       padding_mode='circular', groups=3)\n",
    "\n",
    "conv_layer.weight.data = gaussian_filter\n",
    "\n",
    "conv_layer.to('cuda')\n",
    "\n",
    "def apply_low_freq_noise(x):\n",
    "    # with torch.no_grad():\n",
    "        \"\"\"Generates low-frequency noise.\"\"\"\n",
    "        noise = 255 * conv_layer(x)\n",
    "        return noise\n",
    "    \n",
    "def gen_low_freq_noise():\n",
    "    with torch.no_grad():\n",
    "        \"\"\"Generates low-frequency noise.\"\"\"\n",
    "        noise = 255*torch.randn(1, 3, 300, 600).cuda()\n",
    "        noise = conv_layer(noise)\n",
    "        return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class PrintPhotographEffect:\n",
    "    def __init__(self):\n",
    "        self.downscale = T.Resize((300, 300))  # Simulate low-res printing\n",
    "        self.upscale = T.Resize((1024, 1024))  # Upscale back to original size\n",
    "        self.add_noise = T.Lambda(self._add_noise)  # Add random noise\n",
    "        self.gaussian_blur = T.GaussianBlur(kernel_size=(5,5), sigma=(1.0, 4.0))  # Blur\n",
    "        self.adjust_contrast = T.Lambda(self._adjust_contrast)  # Contrast adjustment\n",
    "        self.vignette = T.Lambda(self._add_vignette)  # Vignette effect\n",
    "        self.jitter = T.ColorJitter(brightness=0.5,contrast=0.5,saturation=0.5)\n",
    "        # self.perspective_transformer = v2.RandomPerspective(distortion_scale=0.2, p=1.0)\n",
    "\n",
    "    def _add_noise(self, img):\n",
    "        rnd_scale = torch.FloatTensor(1).uniform_(0.01, 0.2).to(img.device)\n",
    "        noise = torch.randn_like(img) * rnd_scale  # Gaussian noise\n",
    "        return torch.clamp(img + noise, 0, 1)\n",
    "\n",
    "    def _adjust_contrast(self, img,factor=1.2):\n",
    "        factor = factor  # Slightly increase contrast\n",
    "        mean = torch.mean(img, dim=(-2, -1), keepdim=True)\n",
    "        return torch.clamp((img - mean) * factor + mean, 0, 1)\n",
    "\n",
    "    def _add_vignette(self, img):\n",
    "        _, height, width = img.shape\n",
    "        y, x = torch.meshgrid(\n",
    "            torch.linspace(-1, 1, height, device=img.device),\n",
    "            torch.linspace(-1, 1, width, device=img.device)\n",
    "        )\n",
    "        vignette = 1 - torch.sqrt(x**2 + y**2)\n",
    "        vignette = vignette.to(img.device).unsqueeze(0)  # Add channel dimension\n",
    "        return torch.clamp(img * vignette, 0, 1)\n",
    "\n",
    "    def __call__(self, img,upscale_first=False):\n",
    "        # img = img.permute(2,1,0)\n",
    "        # img = self.downscale(img)\n",
    "        # if upscale_first:\n",
    "        #   img = self.upscale(img)\n",
    "\n",
    "        img = self.add_noise(img)\n",
    "\n",
    "        img = self.gaussian_blur(img)\n",
    "\n",
    "        # img = self.adjust_contrast(img)\n",
    "\n",
    "        img = self.jitter(img)\n",
    "        # img = self.vignette(img)\n",
    "\n",
    "        # img = self.perspective_transformer(img)\n",
    "        # if not upscale_first:\n",
    "        #   img = self.upscale(img)\n",
    "\n",
    "        return img#.permute(2,1,0)\n",
    "\n",
    "effect = PrintPhotographEffect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_ref_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_t = torch.tensor(H).float().unsqueeze(0).cuda()\n",
    "def warp(img):\n",
    "    return kornia.geometry.transform.warp_perspective(img, H_t, (orig_ref_frame.shape[0], orig_ref_frame.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_dst = kornia.geometry.transform.warp_perspective(img_tensor.unsqueeze(0), torch.tensor(H).float().unsqueeze(0).cuda(), (frame.shape[0], frame.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(torch_dst.cpu()[0].permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_patch = torch.rand(1, 3, 300, 600).cuda() - 0.5\n",
    "adv_patch.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_patch_lpf = apply_low_freq_noise(adv_patch)[0].detach().unsqueeze(0).clamp(0, 0.3922)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_patch_lpf.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(adv_patch_lpf).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_patch_lpf_aug = net(adv_patch_lpf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w  =warp(adv_patch_lpf_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_patch_lpf[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = w.cpu()[0].permute(1, 2, 0).numpy()\n",
    "plt.imshow(adv_patch_lpf.cpu()[0].permute(1, 2, 0).numpy())\n",
    "plt.show()\n",
    "plt.imshow(adv_patch_lpf_aug.cpu()[0].permute(1, 2, 0).numpy())\n",
    "plt.show()\n",
    "plt.imshow(effect(adv_patch_lpf_aug).cpu()[0].permute(1, 2, 0).numpy())\n",
    "plt.show()\n",
    "plt.imshow(wt)\n",
    "plt.show()\n",
    "plt.imshow( ((wt != 0) * -0.5 + 1) * orig_ref_frame / 255 + wt*0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_resizer = torchvision.transforms.Resize((52, 92))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(net(adv_patch_lpf)[0].detach().cpu().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot hist of adv_patch\n",
    "# plt.hist(adv_patch.detach().cpu().numpy().flatten(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_tensor = torch.tensor(orig_ref_frame.transpose(2,0,1)).float().cuda().unsqueeze(0) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adv_patch = torch.rand(1, 3, 300, 600).cuda() - 0.5\n",
    "# adv_patch.requires_grad = True\n",
    "\n",
    "resizer =  torchvision.transforms.Resize((300, 600))\n",
    "latent = torch.rand((1,4, 16, 16), device=device, requires_grad=True)\n",
    "\n",
    "opt = torch.optim.Adam([latent], lr=0.5)\n",
    "# opt = torch.optim.SGD([latent], lr=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(vae.decode(latent).sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_patch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = torch.optim.Adam([adv_patch], lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_patch.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # aug = 'None'\n",
    "    # if np.random.rand() > 0.2:\n",
    "    #     if np.random.rand() > 0.5:\n",
    "    #         adv_patch_lpf_aug = net(adv_patch_lpf)\n",
    "    #         aug = 'net'\n",
    "    #     else:\n",
    "    #         adv_patch_lpf_aug = effect(adv_patch_lpf)\n",
    "    #         aug = 'effect'\n",
    "    # else:\n",
    "    #     adv_patch_lpf_aug = adv_patch_lpf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_func(adv_patch_lpf,type):\n",
    "    if type == 'net':\n",
    "        return net(adv_patch_lpf)\n",
    "    if type == 'effect':\n",
    "        return effect(adv_patch_lpf)\n",
    "    return adv_patch_lpf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_clases = [torch.tensor([x]*batch_size).cuda() for x in [817, 705, 609, 586, 436, 627, 468, 621, 803, 407, 408, 751, 717, 864,866, 661]]\n",
    "target_class = torch.tensor([980]).cuda()\n",
    "critereon = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def adv_loss_calc(image):\n",
    "    adv_loss = 0\n",
    "    pred = resnet_predict_raw(image)\n",
    "    if not target_class:\n",
    "      for p in pred:\n",
    "        adv_loss += torch.stack([p[c.item()] for c in orig_clases]).mean() / pred.shape[0]\n",
    "      return adv_loss\n",
    "    else:\n",
    "      adv_loss =  critereon(pred, target_class) / 100\n",
    "    return adv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mask  =warp(adv_patch_lpf*0+1)\n",
    "w  =warp(adv_patch_lpf)\n",
    "sum_tensor =  ((w_mask != 0) * -0.5 + 1) * ref_tensor + w * 0.5\n",
    "\n",
    "plt.imshow(sum_tensor.detach().cpu()[0].permute(1,2,0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 760188/990000 [15:54:41<4:48:36, 13.27it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 40\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# sum_tensor =  ((w != 0) * -0.5 + 1) * ref_tensor + w*0.5\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# sum_tensor =  ((w_mask != 0) * -1 + 1) * ref_tensor + w\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# sum_tensor =  ((w_mask != 0) * -0.5 + 1) * ref_tensor + w * 0.5\u001b[39;00m\n\u001b[1;32m     39\u001b[0m sum_tensor \u001b[38;5;241m=\u001b[39m  ((w_mask \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39mtransperancy \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m ref_tensor \u001b[38;5;241m+\u001b[39m w \u001b[38;5;241m*\u001b[39m transperancy\n\u001b[0;32m---> 40\u001b[0m sum_tensor_aug \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43msum_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m loss \u001b[38;5;241m=\u001b[39m adv_loss_calc(sum_tensor_aug\u001b[38;5;241m.\u001b[39mcuda()) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m+\u001b[39m patch_norm\n\u001b[1;32m     44\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m, in \u001b[0;36mLoRA_U_Net.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 25\u001b[0m     o \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m x  \u001b[38;5;66;03m# Residual Learning\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# o = (o - o.min()) / (o.max() - o.min())  # Normalize to [0, 1]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m o\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/models/segmentation/_utils.py:23\u001b[0m, in \u001b[0;36m_SimpleSegmentationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# contract: features is a dict of tensors\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m result \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:69\u001b[0m, in \u001b[0;36mIntermediateLayerGetter.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m out \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 69\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_layers:\n\u001b[1;32m     71\u001b[0m         out_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_layers[name]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/models/mobilenetv3.py:111\u001b[0m, in \u001b[0;36mInvertedResidual.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 111\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_res_connect:\n\u001b[1;32m    113\u001b[0m         result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1918\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1909\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1911\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1918\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1919\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1920\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "for i in tqdm.tqdm(range(10000,1000000)):\n",
    "\n",
    "    opt.zero_grad()\n",
    "\n",
    "    width_aug = torch.rand(1)[0] * 200 - 100\n",
    "    height_aug = torch.rand(1)[0] * 200 - 100\n",
    "\n",
    "    resizer_aug =  torchvision.transforms.Resize((300 - int(height_aug), 600 - int(width_aug)))                     \n",
    "\n",
    "    adv_patch = vae.decode(latent).sample.clamp(0,0.3922)\n",
    "    \n",
    "    patch_norm = adv_patch.norm() / 20\n",
    "    # adv_patch = adv_patch / adv_patch.std()\n",
    "\n",
    "    # adv_patch = adv_patch * 0.3922\n",
    "    # adv_patch_norm = (adv_patch - adv_patch.mean()) / adv_patch.std()\n",
    "    \n",
    "\n",
    "\n",
    "    rnd_amp = (torch.rand(1)[0] ) * 5\n",
    "    adv_patch_lpf = resizer_aug(rnd_amp * effect(adv_patch))#apply_low_freq_noise(adv_patch_norm )\n",
    "    # with chance 0.5 augment\n",
    "    # tmp_aug = []\n",
    "    # augs = ['net','effect','none']\n",
    "    # for aug in augs:\n",
    "    #     tmp_aug.append(aug_func(adv_patch_lpf,aug))\n",
    "\n",
    "\n",
    "    # adv_patch_lpf_aug = torch.stack(tmp_aug,1)[0]\n",
    "\n",
    "    w_mask  =warp(adv_patch_lpf*0+1)\n",
    "    w  =warp(adv_patch_lpf)\n",
    "\n",
    "    transperancy = torch.rand(1)[0].clamp(0.01,1)\n",
    "    # sum_tensor =  ((w != 0) * -0.5 + 1) * ref_tensor + w*0.5\n",
    "    # sum_tensor =  ((w_mask != 0) * -1 + 1) * ref_tensor + w\n",
    "    # sum_tensor =  ((w_mask != 0) * -0.5 + 1) * ref_tensor + w * 0.5\n",
    "    sum_tensor =  ((w_mask != 0) * -transperancy + 1) * ref_tensor + w * transperancy\n",
    "    sum_tensor_aug = net(sum_tensor)\n",
    "\n",
    "\n",
    "    loss = adv_loss_calc(sum_tensor_aug.cuda()) * 1000 + patch_norm\n",
    "    loss.backward()\n",
    "    grad_norm = latent.grad.norm()#adv_patch.grad.norm()\n",
    "    opt.step()\n",
    "    if i % 10000 == 0:\n",
    "            #plot the patch\n",
    "            plt.imshow(adv_patch_lpf.detach().cpu()[0].permute(1,2,0))\n",
    "            plt.show()\n",
    "            \n",
    "            plt.imshow(sum_tensor_aug.cpu().detach().squeeze().permute(1,2,0))\n",
    "            # add title\n",
    "            plt.show()\n",
    "\n",
    "            plt.imshow(sum_tensor.cpu().detach().squeeze().permute(1,2,0))\n",
    "            # add title\n",
    "            plt.show()\n",
    "            print(loss.item(),grad_norm)\n",
    "            print(resnet_predict(sum_tensor_aug.cuda()))\n",
    "            print(resizer_aug)\n",
    "            print(transperancy)\n",
    "            print(rnd_amp)\n",
    "            adv_patch_r = resizer(adv_patch)\n",
    "            plt.imshow(adv_patch_r[0].detach().cpu().permute(1,2,0))\n",
    "            plt.axis('off')\n",
    "            plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "            plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "            plt.gcf().set_facecolor(\"black\")\n",
    "            plt.show()\n",
    "\n",
    "            adv_patch_r = resizer(adv_patch)\n",
    "            plt.imshow(adv_patch_r[0].clamp(0,0.392).detach().cpu().permute(1,2,0))\n",
    "            plt.axis('off')\n",
    "            plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "            plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "            plt.gcf().set_facecolor(\"black\")\n",
    "            plt.show()\n",
    "#     if i % 1000 == 0:\n",
    "#         with torch.no_grad():\n",
    "#                 # adv_patch_norm = (adv_patch - adv_patch.mean()) / adv_patch.std()\n",
    "#                 adv_patch_lpf = adv_patch#apply_low_freq_noise(adv_patch_norm ).clamp(0, 0.3922)\n",
    "#                 # adv_patch_lpf_aug = effect(adv_patch_lpf)\n",
    "                \n",
    "#                 w_mask  =warp(adv_patch_lpf*0+1)\n",
    "#                 w  =warp(adv_patch_lpf)\n",
    "\n",
    "#                 # sum_tensor =  ((w != 0) * -0.8 + 1) * ref_tensor + w*0.8\n",
    "#                 # sum_tensor =  ((w_mask != 0) * -1 + 1) * ref_tensor + w\n",
    "#                 sum_tensor =  ((w_mask != 0) * -0.5 + 1) * ref_tensor + w * 0.5\n",
    "#                 sum_tensor_aug = net(sum_tensor)\n",
    "\n",
    "\n",
    "        # print(loss.item())\n",
    "        # print(resnet_predict(sum_tensor_aug.cuda()))\n",
    "        # plt.imshow(sum_tensor_aug.cpu().detach().squeeze().permute(1,2,0))\n",
    "        # plt.show()\n",
    "            # plt.imshow(adv_patch_lpf[0].detach().cpu().permute(1,2,0))\n",
    "            # plt.show()\n",
    "            # plt.imshow(net(adv_patch_lpf)[0].detach().cpu().permute(1,2,0))\n",
    "            # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(net(orig_ref_frame_tensor).cpu().detach().squeeze().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sum_tensor.cpu().detach().squeeze().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack([100*p[c.item()] for c in orig_clases])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sum_tensor.cpu().detach().squeeze().permute(1,2,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(adv_patch_lpf_aug.detach().cpu().numpy().squeeze().transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(adv_patch[0].detach().cpu().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_patch.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_patch_lpf = apply_low_freq_noise(adv_patch ).clamp(0, 0.3922)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_patch_lpf.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_patch_lpf.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save adv_patch_lpf to adv_patch_lpf.png\n",
    "adv_patch_r = resizer(adv_patch)\n",
    "plt.imshow(adv_patch_r[0].clamp(0,0.392).detach().cpu().permute(1,2,0))\n",
    "plt.axis('off')\n",
    "plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "plt.gcf().set_facecolor(\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pillow import Image\n",
    "img = Image.fromarray(img_adv_patch_lpf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(adv_patch_lpf_clamp[0].detach().cpu().permute(1,2,0))\n",
    "# remove axes\n",
    "plt.axis('off')\n",
    "plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "plt.gcf().set_facecolor(\"red\")\n",
    "plt.show()\n",
    "# make background red and narrow borders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_patch_lpf = apply_low_freq_noise(adv_patch ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_adv_patch_lpf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(net(adv_patch_lpf).cpu().detach().squeeze().permute(1,2,0))\n",
    "plt.show()\n",
    "plt.imshow(adv_patch_lpf.cpu().detach().squeeze().permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(adv_patch.cpu().detach().squeeze().permute(1,2,0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
