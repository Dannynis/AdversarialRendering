{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if this colab environment\n",
    "on_colab = 'google.colab' in str(get_ipython())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if on_colab:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/NVIDIAGameWorks/kaolin/raw/4d8f49dd3e617cb1fec3e2c2bc3b2903ff952321/examples/samples/rendered_clock.zip\n",
    "# !unzip rendered_clock.zip\n",
    "if on_colab:\n",
    "  !git clone https://github.com/NVIDIAGameWorks/kaolin.git\n",
    "  !pip install kaolin==0.16.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.4.0_cu121.html --quiet\n",
    "  !pip install torch torchvision transformers diffusers pillow  --quiet\n",
    "  !pip install objaverse --quiet\n",
    "  !pip install lpips\n",
    "  !pip install -U \"comet_ml>=3.44.0\" --quiet\n",
    "  !cp /content/kaolin/kaolin/render/lighting/sg.py /usr/local/lib/python3.10/dist-packages/kaolin/render/lighting/sg.py\n",
    "# !wget https://raw.githubusercontent.com/NVIDIAGameWorks/kaolin/bcbc92f4ed6f176e9d320932ea5ee1262e2ce059/examples/samples/sphere.obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if on_colab:\n",
    "  checkpoint_root = '/content/drive/MyDrive/Adver_diff_checkpoints/'\n",
    "else:\n",
    "  checkpoint_root = '/home/dcor/niskhizov/adv_diff_checkpoints/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title comet stuff\n",
    "import comet_ml\n",
    "from comet_ml.integration.pytorch import watch\n",
    "\n",
    "comet_ml.login(project_name=\"Adversarial-diff-rendering-notebook-V3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warp 1.4.2 initialized:\n",
      "   CUDA Toolkit 12.6, Driver 12.4\n",
      "   Devices:\n",
      "     \"cpu\"      : \"x86_64\"\n",
      "     \"cuda:0\"   : \"NVIDIA GeForce RTX 3090\" (24 GiB, sm_86, mempool enabled)\n",
      "   Kernel cache:\n",
      "     /home/dcor/niskhizov/.cache/warp/1.4.2\n"
     ]
    }
   ],
   "source": [
    "import kaolin\n",
    "import kaolin as kal\n",
    "import pickle\n",
    "import torch\n",
    "import objaverse\n",
    "\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from kaolin.render.lighting import SgLightingParameters\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  dynamic light, camera rendering funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "IMAGE_SIZE = 1024\n",
    "\n",
    "def make_camera(eye):\n",
    "  return kal.render.camera.Camera.from_args(eye=torch.tensor(eye),\n",
    "                                         at=torch.tensor([0., 0., 0.]),\n",
    "                                         up=torch.tensor([0., 1., 0]),\n",
    "                                         fov=math.pi * 45 / 180,\n",
    "                                            near=0.1, far=10000.,\n",
    "                                         width=IMAGE_SIZE,\n",
    "                                            height=IMAGE_SIZE,\n",
    "                                            device='cuda')\n",
    "forbidden_theta = []\n",
    "forbidden_phi = []\n",
    "\n",
    "resolution = 20\n",
    "for theta in np.linspace(1, 0.9, 3):\n",
    "    forbidden_theta.append(theta)\n",
    "\n",
    "for phi in np.linspace(0.001, 3, resolution):\n",
    "    forbidden_phi.append(phi)\n",
    "\n",
    "forbidden_theta = set(forbidden_theta)\n",
    "forbidden_phi = set(forbidden_phi)\n",
    "\n",
    "def random_polar(r_range, phi_range, theta_range):\n",
    "  done = False\n",
    "  while not done:\n",
    "    r = np.random.uniform(r_range[0], r_range[1])\n",
    "    theta = np.random.uniform(theta_range[0], theta_range[1])\n",
    "    phi = np.random.uniform(phi_range[0], phi_range[1])\n",
    "    if theta not in forbidden_theta and phi not in forbidden_phi:\n",
    "      done = True\n",
    "  return [r, theta, phi]\n",
    "\n",
    "\n",
    "def polar_to_cartesian(r, phi, theta):\n",
    "  y = r * math.cos(theta)\n",
    "  z = r * math.sin(theta) * math.cos(phi)\n",
    "  x = r * math.sin(theta) * math.sin(phi)\n",
    "  return [x,y,z]\n",
    "\n",
    "def random_light(strength_range = [8,15],suns_range=[1, 5], phi_range=[0, math.pi * 2], theta_range=[0, math.pi / 2]):\n",
    "  n_suns = int(np.random.uniform(suns_range[0],suns_range[1]))\n",
    "  light_directions = []\n",
    "  for i in range(n_suns):\n",
    "    [r, theta, phi] = random_polar(r_range=[1, 5], phi_range=phi_range, theta_range=theta_range)\n",
    "    direction = np.array(polar_to_cartesian(r, phi, theta))\n",
    "    direction = direction / np.sqrt(np.sum(direction * direction))\n",
    "    light_directions.append(direction)\n",
    "\n",
    "  light_directions = torch.tensor(np.array(light_directions)).cuda()\n",
    "\n",
    "  strength = np.random.uniform(strength_range[0],strength_range[1])\n",
    "  lighting = SgLightingParameters.from_sun(light_directions.float(), strength).cuda()\n",
    "\n",
    "  return lighting, (strength,light_directions)\n",
    "\n",
    "def polar_camera_and_light(r, phi, theta):\n",
    "  eye = polar_to_cartesian(r, phi, theta)\n",
    "  camera = make_camera(eye)\n",
    "  eye = np.array(eye)\n",
    "  eye_norm = np.sqrt(np.sum(eye * eye))\n",
    "\n",
    "  n_suns = int(np.random.uniform(1, 5))\n",
    "  light_directions = []\n",
    "  light_direction = torch.tensor(eye / eye_norm, dtype=torch.float32).view(1, 1, 3).cuda()\n",
    "  strength = np.random.uniform(4, 10)\n",
    "  lighting = SgLightingParameters.from_sun(light_direction.float(), strength).cuda()\n",
    "  return camera, lighting\n",
    "\n",
    "theta_eps = 0.3\n",
    "\n",
    "def random_camera_and_light(r_range = [0, 5], phi_range=[0, math.pi * 2], theta_range=[ math.pi / 2 - theta_eps,0]):\n",
    "  [r, theta, phi] = random_polar(r_range, phi_range, theta_range)\n",
    "  return polar_camera_and_light(r, phi, theta), (r, phi, theta)\n",
    "\n",
    "\n",
    "def render(in_cam, mesh, lighting, pbr_mat=None):\n",
    "    if pbr_mat is not None:\n",
    "      render_res = kal.render.easy_render.render_mesh(in_cam, mesh, lighting=lighting, custom_materials = [pbr_mat])\n",
    "    else:\n",
    "      render_res = kal.render.easy_render.render_mesh(in_cam, mesh, lighting=lighting)\n",
    "    img = render_res[kal.render.easy_render.RenderPass.render].squeeze(0).clamp(0, 1)\n",
    "    return img\n",
    "\n",
    "# Show simple render"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lpf noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_low_frequency_noise_fft(noise, cutoff_ratio: float = 0.05):\n",
    "    \"\"\"Generates low-frequency noise using FFT-based filtering.\"\"\"\n",
    "    # Generate random white noise\n",
    "    size = noise.shape[0]\n",
    "    # Perform FFT to get frequency domain representation\n",
    "    noise_fft = torch.fft.fft2(noise)\n",
    "\n",
    "    # Shift the zero frequency component to the center\n",
    "    noise_fft_shifted = torch.fft.fftshift(noise_fft)\n",
    "\n",
    "    # Create a low-pass filter (circle mask in frequency domain)\n",
    "    mask = torch.zeros_like(noise_fft_shifted)\n",
    "    center = size // 2\n",
    "    cutoff = int(center * cutoff_ratio)  # Controls how much low frequency is kept\n",
    "\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            if (i - center) ** 2 + (j - center) ** 2 < cutoff ** 2:\n",
    "                mask[i, j] = 1\n",
    "\n",
    "    # Apply the mask to the FFT coefficients\n",
    "    low_freq_fft = noise_fft_shifted * mask\n",
    "\n",
    "    # Inverse FFT to convert back to spatial domain\n",
    "    low_freq_fft_shifted_back = torch.fft.ifftshift(low_freq_fft)\n",
    "    low_freq_noise = torch.fft.ifft2(low_freq_fft_shifted_back).real\n",
    "\n",
    "    return low_freq_noise\n",
    "\n",
    "class SmoothNoise(nn.Module):\n",
    "  def __init__(self, noise_shape):\n",
    "      super().__init__()\n",
    "      # generate noise parameter for each of the 3 channels\n",
    "\n",
    "      self.noise_parameter = nn.ParameterList([nn.Parameter(torch.randn(noise_shape,noise_shape)) for i in range(3)])\n",
    "\n",
    "  def forward(self, texture_map):\n",
    "      all_channels_noise =  torch.stack([generate_low_frequency_noise_fft(p) for p in self.noise_parameter])\n",
    "      if texture_map.shape[-1] == 3:\n",
    "        all_channels_noise = all_channels_noise.T\n",
    "      output = texture_map + all_channels_noise\n",
    "      return output\n",
    "\n",
    "\n",
    "def gaussian_kernel(size: int, sigma: float):\n",
    "    \"\"\"Generates a 2D Gaussian kernel.\"\"\"\n",
    "    coords = torch.arange(size).float()\n",
    "    coords -= (size - 1) / 2.0\n",
    "    g = torch.exp(-coords.pow(2) / (2 * sigma ** 2))\n",
    "    g = g / g.sum()  # Normalize\n",
    "    g_2d = g.unsqueeze(0) * g.unsqueeze(1)  # Outer product to create 2D kernel\n",
    "    return g_2d\n",
    "\n",
    "# Create Gaussian kernel\n",
    "kernel_size = 11  # Kernel size (odd number)\n",
    "sigma = 10        # Standard deviation of the Gaussian\n",
    "num_noises = 1\n",
    "gaussian_filter = gaussian_kernel(kernel_size, sigma).unsqueeze(0).repeat(3,1,1).unsqueeze(1)\n",
    "\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=kernel_size,\n",
    "                       padding=kernel_size // 2, bias=False,\n",
    "                       padding_mode='circular', groups=3)\n",
    "\n",
    "# Initialize the Conv2d weights with the Gaussian kernel\n",
    "with torch.no_grad():\n",
    "    conv_layer.weight = nn.Parameter(gaussian_filter)\n",
    "\n",
    "#freeze conv_layer parameters\n",
    "for param in conv_layer.parameters():\n",
    "  param.requires_grad = False\n",
    "\n",
    "class GNSmoothNoise(nn.Module):\n",
    "  def __init__(self, noise_shape, num_noises=num_noises, zero_init=True):\n",
    "      super().__init__()\n",
    "      # generate noise parameter for each of the 3 channels\n",
    "      self.kernel_size = kernel_size\n",
    "      self.noise_parameter = nn.Parameter(torch.randn(num_noises,3,noise_shape,noise_shape)*5)\n",
    "      if zero_init:\n",
    "        self.noise_parameter.data = torch.zeros_like(self.noise_parameter) - 10\n",
    "\n",
    "      self.conv_layer = conv_layer\n",
    "\n",
    "\n",
    "  def forward(self, texture_map):\n",
    "      return texture_map + conv_layer(self.noise_parameter).sum(0) * 0.1#(texture_map +  torch.sigmoid(self.conv_layer(self.noise_parameter).sum(0) ))/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Photographic augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title augmentation\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class PrintPhotographEffect:\n",
    "    def __init__(self):\n",
    "        self.downscale = T.Resize((300, 300))  # Simulate low-res printing\n",
    "        self.upscale = T.Resize((1024, 1024))  # Upscale back to original size\n",
    "        self.add_noise = T.Lambda(self._add_noise)  # Add random noise\n",
    "        self.gaussian_blur = T.GaussianBlur(kernel_size=(5,5), sigma=(1.0, 4.0))  # Blur\n",
    "        self.adjust_contrast = T.Lambda(self._adjust_contrast)  # Contrast adjustment\n",
    "        self.vignette = T.Lambda(self._add_vignette)  # Vignette effect\n",
    "        self.jitter = T.ColorJitter(brightness=0.5,contrast=0.5,saturation=0.5)\n",
    "        # self.perspective_transformer = v2.RandomPerspective(distortion_scale=0.2, p=1.0)\n",
    "\n",
    "    def _add_noise(self, img):\n",
    "        rnd_scale = torch.FloatTensor(1).uniform_(0.01, 0.2).to(img.device)\n",
    "        noise = torch.randn_like(img) * rnd_scale  # Gaussian noise\n",
    "        return torch.clamp(img + noise, 0, 1)\n",
    "\n",
    "    def _adjust_contrast(self, img,factor=1.2):\n",
    "        factor = factor  # Slightly increase contrast\n",
    "        mean = torch.mean(img, dim=(-2, -1), keepdim=True)\n",
    "        return torch.clamp((img - mean) * factor + mean, 0, 1)\n",
    "\n",
    "    def _add_vignette(self, img):\n",
    "        _, height, width = img.shape\n",
    "        y, x = torch.meshgrid(\n",
    "            torch.linspace(-1, 1, height, device=img.device),\n",
    "            torch.linspace(-1, 1, width, device=img.device)\n",
    "        )\n",
    "        vignette = 1 - torch.sqrt(x**2 + y**2)\n",
    "        vignette = vignette.to(img.device).unsqueeze(0)  # Add channel dimension\n",
    "        return torch.clamp(img * vignette, 0, 1)\n",
    "\n",
    "    def __call__(self, img,upscale_first=False):\n",
    "        img = img.permute(2,1,0)\n",
    "        img = self.downscale(img)\n",
    "        if upscale_first:\n",
    "          img = self.upscale(img)\n",
    "\n",
    "        img = self.add_noise(img)\n",
    "\n",
    "        img = self.gaussian_blur(img)\n",
    "\n",
    "        # img = self.adjust_contrast(img)\n",
    "\n",
    "        img = self.jitter(img)\n",
    "        # img = self.vignette(img)\n",
    "\n",
    "        # img = self.perspective_transformer(img)\n",
    "        if not upscale_first:\n",
    "          img = self.upscale(img)\n",
    "\n",
    "        return img.permute(2,1,0)\n",
    "\n",
    "effect = PrintPhotographEffect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resnet classfier init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torchvision\n",
    "\n",
    "\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "model = resnet50(weights=weights)\n",
    "model = model.eval().cuda()\n",
    "\n",
    "\n",
    "preprocess = weights.transforms()\n",
    "#input image w x h x c\n",
    "\n",
    "def resnet_predict(image):\n",
    "  with torch.no_grad():\n",
    "    prediction = resnet_predict_raw(image)\n",
    "    prediction = prediction.squeeze(0).softmax(0)\n",
    "    class_id = prediction.argmax().item()\n",
    "    score = prediction[class_id].item()\n",
    "    category_name = weights.meta[\"categories\"][class_id]\n",
    "    # return(f\"class id - {class_id} {category_name}: {100 * score:.1f}%\")\n",
    "    return(f\"{category_name}: {100 * score:.1f}%\")\n",
    "\n",
    "\n",
    "\n",
    "def resnet_predict_raw(image):\n",
    "\n",
    "  image = image.permute(2, 0, 1)\n",
    "\n",
    "  if image.shape != (3, 256, 256):\n",
    "    rimage = torchvision.transforms.Resize((256, 256))(image)\n",
    "  else:\n",
    "    rimage = image\n",
    "\n",
    "  # Step 3: Apply inference preprocessing transforms\n",
    "  batch = preprocess(rimage).unsqueeze(0)\n",
    "\n",
    "  # Step 4: Use the model and print the predicted category\n",
    "  return model(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### objaverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x,y in zip(range(len(weights.meta[\"categories\"])),weights.meta[\"categories\"]):\n",
    "#   print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dcor/niskhizov/anaconda3/lib/python3.12/site-packages/kaolin/io/gltf.py:285: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:1560.)\n",
      "  output = torch.frombuffer(\n"
     ]
    }
   ],
   "source": [
    "import objaverse\n",
    "\n",
    "uids = ['f53d75bd123b40bca14d12d54286f432']\n",
    "\n",
    "objaverse.load_objects(\n",
    "    uids,\n",
    "    download_processes = 1\n",
    ")\n",
    "\n",
    "orig_mesh = kal.io.gltf.import_mesh(f'{os.path.expanduser(\"~\")}/.objaverse/hf-objaverse-v1/glbs/000-027/f53d75bd123b40bca14d12d54286f432.glb')\n",
    "orig_mesh.vertices = kal.ops.pointcloud.center_points(orig_mesh.vertices.unsqueeze(0), normalize=True).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "mesh = copy.deepcopy(orig_mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lighting.amplitude.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lighting.from_sun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.153110473592704 tensor([[ 0.5355,  0.7162,  0.4476],\n",
      "        [ 0.9066,  0.4181, -0.0578],\n",
      "        [ 0.1971,  0.5192,  0.8316],\n",
      "        [ 0.0539,  0.9969, -0.0573]], device='cuda:0', dtype=torch.float64)\n",
      "2.096493342583905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dcor/niskhizov/anaconda3/lib/python3.12/site-packages/kaolin/render/easy_render/mesh.py:282: UserWarning: Using torch.cross without specifying the dim arg is deprecated.\n",
      "Please either pass the dim explicitly or simply use torch.linalg.cross.\n",
      "The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at ../aten/src/ATen/native/Cross.cpp:62.)\n",
      "  im_bitangents = torch.nn.functional.normalize(torch.cross(im_tangents, im_base_normals), dim=-1)\n",
      "/home/dcor/niskhizov/anaconda3/lib/python3.12/site-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/tmp/ipykernel_415595/1163450222.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  resnet_predict(torch.tensor(img).cuda())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'orange: 25.0%'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(camera, _), (r, phi, theta) = random_camera_and_light(r_range = [1.5, 4])#,phi_range=[ 0.2,0.2] ,theta_range=[math.pi / 2 - theta_eps, math.pi / 2 - theta_eps])\n",
    "lighting, (strength,light_directions) = random_light(strength_range=[20,30])#kal.render.easy_render.default_lighting().cuda()\n",
    "print(strength,light_directions)\n",
    "# lighting.amplitude = torch.ones(1,3).cuda() * 8\n",
    "\n",
    "print(r)\n",
    "\n",
    "\n",
    "# mesh.materials[0].diffuse_texture = mesh.materials[0].diffuse_texture\n",
    "\n",
    "img = render(camera, mesh.cuda(), lighting.cuda())\n",
    "\n",
    "plt.imshow(img.detach().cpu())\n",
    "\n",
    "\n",
    "resnet_predict(torch.tensor(img).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 1\n",
    "\n",
    "# analog_clock_class = torch.tensor([409]*batch_size).cuda()\n",
    "# wall_clock_class = torch.tensor([892]*batch_size).cuda()\n",
    "# orig_class = torch.tensor([948]*batch_size).cuda()\n",
    "orig_clases = [torch.tensor([x]*batch_size).cuda() for x in range(948,959)]\n",
    "\n",
    "\n",
    "\n",
    "critereon = torch.nn.CrossEntropyLoss()\n",
    "adv_direction = 100 #None#953# pineapple #None\n",
    "\n",
    "\n",
    "\n",
    "if adv_direction:\n",
    "  directed_class = torch.tensor([adv_direction]*batch_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SN = GNSmoothNoise(1024, zero_init=False).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load saved weights\n",
    "find the most recent file in /content/drive/MyDrive/Adver_diff_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import glob\n",
    "# import re\n",
    "# import os\n",
    "# import torch\n",
    "# # weights_paths = glob.glob('/content/drive/MyDrive/Adver_diff_checkpoints/*_without_sds_bs_20.pt')\n",
    "# # weights_paths.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "# # print(weights_paths[-1])\n",
    "\n",
    "# files = glob.glob('/home/dcor/niskhizov/adv_diff_checkpoints//*.pt')\n",
    "# files.sort(key=os.path.getmtime)\n",
    "# files\n",
    "\n",
    "# SN.noise_parameter = torch.load(files[-1]).noise_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig_mesh.materials[0].diffuse_texture.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sn = SN(torch.zeros(3,1024,1024).cuda()).cpu().detach()\n",
    "# print(sn[0]==sn[1])\n",
    "# # plt.imshow(SN(torch.zeros(3,1024,1024).cuda()).cpu().detach()[2])\n",
    "# plt.imshow(SN(torch.zeros(3,1024,1024).cuda()).cpu().detach().permute(2,1,0))\n",
    "# # plt.imshow(SN.noise_parameter.data[5].cpu().permute(2,1,0))\n",
    "# # plt.imshow(orig_mesh.materials[0].diffuse_texture,aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physical testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ea425b809e0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load 20241130_202829.jpg \n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load the image\n",
    "image_path = '20241130_204238.jpg'  # Replace with the path to your image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "im = image.convert('RGB')\n",
    "img = torch.tensor(np.array(im))\n",
    "img = img / 255\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ea425b27290>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'black swan: 61.7%'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_predict(img.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze all parameters of resnet model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = model.to(device)\n",
    "for param in model.parameters():\n",
    "  param.requires_grad = False\n",
    "\n",
    "import copy\n",
    "mesh = copy.deepcopy(orig_mesh).to(device)\n",
    "mesh2 = copy.deepcopy(orig_mesh).to(device)\n",
    "orig_mesh = orig_mesh.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "weights_paths = glob.glob('/home/dcor/niskhizov/adv_diff_checkpoints//*1_sds_bs_1_sds_only_blsw_photo_augmentations*')\n",
    "weights_paths.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "for w in weights_paths[-10:]:\n",
    "    print(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SN.noise_parameter = torch.load( weights_paths[-1]).noise_parameter\n",
    "SN.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SN2 = SN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (camera, _), (r, phi, theta) = random_camera_and_light(r_range = [1.8, 2.5], phi_range=[3, math.pi * 2], theta_range=[ math.pi / 2 - theta_eps,1])\n",
    "\n",
    "lighting, _ = random_light(strength_range=[15,15],suns_range=[7, 7])\n",
    "# lighting.amplitude = torch.ones(1,3).cuda() * 10\n",
    "mesh.materials[0].diffuse_texture = SN(orig_mesh.materials[0].diffuse_texture.T.to(device)).T\n",
    "mesh2.materials[0].diffuse_texture = SN2(orig_mesh.materials[0].diffuse_texture.T.to(device)).T\n",
    "\n",
    "resolution = 20 # 100\n",
    "plots = []\n",
    "for theta in np.linspace(1, 0.9, 3):\n",
    "  for phi in np.linspace(0.001, 3, resolution):\n",
    "    camera, _ = polar_camera_and_light(1.7, phi, theta)\n",
    "    # print(phi,theta)\n",
    "    with torch.no_grad():\n",
    "      img = render(camera, mesh, lighting)\n",
    "      img2 = render(camera, mesh2, lighting)\n",
    "      orig_img = render(camera, orig_mesh, lighting)\n",
    "\n",
    "      img_score = resnet_predict(torch.tensor(img).cuda())\n",
    "      img2_score = resnet_predict(torch.tensor(img2).cuda())\n",
    "      orig_img_score = resnet_predict(torch.tensor(orig_img).cuda())\n",
    "    # plot both images in the same figure\n",
    "    # put the score as the images title\n",
    "    fig, axs = plt.subplots(1, 3)\n",
    "    axs[0].imshow(img.detach().cpu())\n",
    "    axs[1].imshow(orig_img.detach().cpu())\n",
    "    axs[2].imshow(img2.detach().cpu())\n",
    "\n",
    "    axs[0].set_title(img_score)\n",
    "    axs[1].set_title(orig_img_score)\n",
    "    axs[2].set_title(img2_score)\n",
    "\n",
    "    plots.append([img,orig_img,img2,img_score,orig_img_score,img2_score])\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots[0][0] \n",
    "# plot plots[0][0]\n",
    "plt.imshow(plots[0][0].detach().cpu())\n",
    "resnet_predict(plots[0][0])\n",
    "\n",
    "import numpy as np\n",
    "img = Image.fromarray(np.uint8(plots[0][0].detach().cpu().numpy()*255), 'RGB')\n",
    "img.save('adv_image.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "def save_plots(plots,t,name='image.gif'):\n",
    "  #make animation out of plots list at the same figure\n",
    "  fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "  # add title to the entire fig\n",
    "  fig.suptitle(t)\n",
    "\n",
    "  def update(frame):\n",
    "    axs[0].imshow(plots[frame][0].detach().cpu())\n",
    "    axs[1].imshow(plots[frame][1].detach().cpu())\n",
    "    axs[2].imshow(plots[frame][2].detach().cpu())\n",
    "\n",
    "    axs[0].set_title('adv image score: ' + plots[frame][3])\n",
    "    axs[1].set_title('orig image score: ' + plots[frame][4])\n",
    "    axs[2].set_title('adv sds image score: ' + plots[frame][5])\n",
    "\n",
    "    return axs\n",
    "\n",
    "  ani = animation.FuncAnimation(fig, update, frames=len(plots), interval=2500, repeat=True)\n",
    "\n",
    "  # Display the animation\n",
    "  # plt.show()\n",
    "  #save the animation\n",
    "  gif_file = name\n",
    "  ani.save(gif_file, writer='pillow', fps=3)\n",
    "\n",
    "  print(f\"Animation saved as {gif_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_plots(plots,t='only_bswn_sds_and_adv',name='only_bswn_sds_and_adv.gif')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
